% Тут используется класс, установленный на сервере Papeeria. На случай, если
% текст понадобится редактировать где-то в другом месте, рядом лежит файл matmex-diploma-custom.cls
% который в момент своего создания был идентичен классу, установленному на сервере.
% Для того, чтобы им воспользоваться, замените matmex-diploma на matmex-diploma-custom
% Если вы работаете исключительно в Papeeria то мы настоятельно рекомендуем пользоваться
% классом matmex-diploma, поскольку он будет автоматически обновляться по мере внесения корректив
%
\documentclass{matmex-diploma}

\usepackage{amsmath}

\begin{document}
% Год, город, название университета и факультета предопределены,
% но можно и поменять.
% Если англоязычная титульная страница не нужна, то ее можно просто удалить.
\filltitle{ru}{
    chair              = {Кафедра информационных систем},
    title              = {Разработка программного компонента для проведения сравнительного анализа биологических данный FAIRE-seq},
    % Здесь указывается тип работы. Возможные значения:
    %   coursework - Курсовая работа
    %   diploma - Диплом специалиста
    %   master - Диплом магистра
    %   bachelor - Диплом бакалавра
    type               = {bachelor},
    position           = {студента},
    group              = 4511,
    author             = {Бутомов Артем Сергеевич},
    supervisorPosition = {},
    supervisor         = {Лебедев Сергей Андреевич},
    reviewerPosition   = {аспирант},
    reviewer           = {Сергушичев Алексей Александрович},
    chairHeadPosition  = {},
    chairHead          = {},
    university         = {Санкт-Петербургский национальный исследовательский университет информационных технологий, механики и оптики},
    faculty            = {Факультет информационных технологий и программирования},
%   city               = {Санкт-Петербург},
%   year               = {2013}
}
\filltitle{en}{
    chair              = {Chair of Information Systems},
    title              = {Development of a software component for a comparative analysis of the biological FAIRE-seq data},
    author             = {Artiom Butomov},
    supervisorPosition = {},
    supervisor         = {Sergei Lebedev},
    reviewerPosition   = {postgraduate},
    reviewer           = {Alexey Sergushichev},
    chairHeadPosition  = {},
    chairHead          = {},
    university         = {Saint Petersburg National Research University of Information Technologies, Mechanics and Optics},
    faculty            = {Faculty of Information Technology and Software Engineering},
}
\maketitle
\tableofcontents
% У введения нет номера главы
\section*{Введение}
ДНК (дезоксирибонуклеиновая кислота) --- длинная двухцепочечная молекула, являющаяся носителем генетической информации в биологических организмах.

Изучать пространственную структуру ДНК организма важно для понимания механизмов регуляции жизнидеятельности клетки.

Формальдегидная изоляция регуляторных элементов с последующим секвенированием (Formaldehyde-Assisted Isolation of Regulatory Elements sequencing, FAIRE-Seq) --- Это биологический протокол, позволяющий находить участки, в которых ДНК доступна для связывания белками. Суть работы метода заключается в том, что на ДНК, выделенную из клетки, "прикрепляют" нуклеосомы с помощью формальдегида. Затем ДНК фрагментируют с помощью ультразвука. После этого происходит "разделение" полученных фрагментов ДНК на две группы: участки связанные с белками и "свободные" участки. Далее "свободные" фрагменты "читают" с помощью секвенатора. И наконец, для каждого прочтения секвенатора определяют место в геноме исследуемого организма, откуда он был прочитан.

В контексте данной работы, геном "разбивается" на неперескающиеся отрезки фиксированной длины, называемые бинами. Подсчитывается количество прочтений, начинающихся внутри каждого отрезка. Таким образом, получатся вектор из неотрицательных целых чисел, именуемый вектором покрытия.

Из вектора покрытия можно сделать предположение о вероятности расплетения региона, чем больше значение элемента вектора, тем с большей вероятностью, что регион, соответствующий элементу, был расплетен.

Однако, рассматриваемый протокол не исключает возможности наличия ошибок в результатах биологического эксперемнта. Неточности метода FAIRE-seq обусловлены следующими моментами:
\begin{enumerate}
\item
Протокол работает с колонией клеток. Таким образом в результатах эксперимента мы видим некоторое среднее состояние по всем клеткам
\item
Этап фиксации не обладает 100\% КПД, то есть некоторые белки могут “отвалиться” 
\item
Этап разделения “свободных” и “связанных” фаз также неточен. Вместе со “свободными" вполне могут попасться и связанные фрагменты
\end{enumerate}

Так как в результате эксперемента появляется шум, данные FAIRE-seq удобно анализировать с помощью вероятностых моделей.

Цель данной работы - разработать математическую модель для проведения сравнительного анализа нескольких эксперементов биологический данных FAIRE-seq, научиться оценивать и контролировать число неверных предсказаний модели. 
\\\\
Для достижения цели были поставлены следующие задачи:
\begin{enumerate}
\item
Изучить предметную область
\item
Предложить несколько вероятностных моделей для сравнения экспериментов FAIRE-seq
\item
Реализовать модели в виде программы на языке Python
\item
Оценить эффективность полученной программы
\end{enumerate}

\section{Предлагаемые модели}
\subsection{Описание задачи}

Пусть $\vec{x}=(x_1,..,x_N)$ - вектор прочтений, построенный из какого-то BAM файла.
Сопоставим каждому наблюдению некоторую метку-состояние $s_n$ из множества базовых состояний $s=\{1,..,S\}$, истинные значения которых не знаем. 

Вероятностная модель позволяет найти наиболее правдоподобную последовательность состояний.

\newcommand{\argmax}{\mathop{\rm arg~max}\limits}
$$\hat{s}_{ML} = \argmax_{i \in {1,..,S}^N} \mathcal{P}(x,s;\theta)$$

\subsection{Смесь многомерных распределений Пуассона}

%Будем считать, что мы работаем с двумя векторами покрытия.
%Тогда наблюдение $x_n$ - это пара чисел, каждое из которых соответсвует одному %вектору $\vec{x}=\{\vec{x_1},\vec{x_2}\}$.

%Пусть $S = \{+, -\}$ — множество состояний, которые мы будем называть "базовыми%" состояниями. Каждое из базовых состояний описывает ситуацию в одном образце, %например, $(+)$ — сигнал есть, $(-)$ - шум или сигнала нет. 

%Для задачи сравнения нам нужно множество состояний, описывающее, что происходит %в каждом из образцов, то есть $S^2 = \{(+, +), (-, -), (+, -), (-, +)\}$.

Будем моделировать количество прочтений вдоль генома с помощью смеси многомерных распределений Пуасона.

Пусть $\pi = (\pi_1,..,\pi_S)$ - априорные вероятности компонент.
$\lambda = (\lambda_1,..,\lambda_S)$ - параметры пуасоновских испусканий для каждой компоненты смеси.

Тогда правдоподобие неполных данных записывается так:

$$p(x;\pi,\lambda) = \prod_{n=1}^N\sum_{i=1}^S\pi_i\mathcal{P}(x_n;\lambda_i)$$

Интерпретация в качестве порождающей модели следующая: сначала случайным образом выбираем скрытое состояние, применяя распределение $\pi$, а затем используем выбранное скрытое состояние для порождения наблюдения.

%% plate диаграмму крепи.

Чтобы найти параметры модели с помощью оценки максимамльного правдоподобия, следует обратить в максимум совместное правдоподобие наблюдаемых и скрытых переменных. Поэтому Методом максимального правдоподобия найти решение довольно сложно, поскольку для этого требуется решить систему, где оцениваемые параметры зависят от наблюдаемой выборки $x$ и неизвестных значений скрытых состояний $s$. Асимптотическая сложность решения возрастет в $S^N$ раз.

Поэтому, будем искать приближенное решение с помощью EM-алгоритма, в котором правдоподобие оптимизируется до сходимости. Последовательность действий формируется следующим образом\cite{book:Bishop}:
\begin{enumerate}
\item
Инициализировать начальные значения параметров
\item
Присвоение ожидаемых значений скрытым переменным при условии текущих оценок параметров и нахождение математического ожидания правдоподобия.
$$\mathrm{Q}(\theta|\theta^{\operatorname{old}})
=\mathrm{E}[\log p(x,s;\theta)]\le \log p(x;\theta)$$
\item
Переоценка параметров с учетом обновленных ожидаемых значений скрытых переменных
$$\theta^{\operatorname{new}}=\argmax_{\theta \in \Theta} \mathrm{Q}(\theta|\theta^{\operatorname{old}})$$
\item
Вычислить логарифм правдоподобия и проверить на сходимость
\end{enumerate}

Выведем EM-алгоритм для смеси многомерных Пуасоновских испусканий.

Запишем логарифм функции правдоподобия:

\begin{equation}
\ln p(x;\pi,\lambda) = \sum_{n=1}^N \ln{\sum_{i=1}^S} \pi_i \mathcal{P}(x_n;\lambda_i)
\end{equation}

\subsection*{Шаг E}

\begin{equation}
\gamma(s_{ni}) = \frac{\pi_i\mathcal{P}(x_n;\lambda_i)}{\sum_{j=1}^S\pi_j\mathcal{P}(x_n;\lambda_j)}
\end{equation}

\subsection*{Шаг М}

Чтобы максимизировать логарифмическое правдоподобие относительно параметров, необходимо взять частные производные и приравнять их к нулю.

% вместо |  - иполльзовать ;
\begin{align*}
\frac{\partial}{\partial \lambda_i}\mathrm{E}[\log p(x;s,\theta)]\\
&=\frac{\partial}{\partial \lambda_i}\sum_{n=1}^N \sum_{s=1}^S
\mathrm{E}[s_{ni}]\{\log \pi_i + \log \mathcal{P}(x_n|\theta)\}\\
&= \sum_{n=1}^N \mathrm{E}[s_{ni}] \frac{\partial}{\partial \lambda_i} \log \mathcal{P}(x_n|\theta)\\
&= \sum_{n=1}^N \gamma(s_{nk}) \frac{\partial}{\partial \lambda_i} \log {\frac{\lambda^{x_n} e^{-\lambda}}{x_n!}}\\
&= \sum_{n=1}^N \gamma(s_{nk}) \frac{\partial}{\partial \lambda_i} (\log {\lambda_i}^{x_n} + \log e^{-\lambda_k} - \log x_n! )\\
&= \sum_{n=1}^N \gamma(s_{ni}) \frac{\partial}{\partial \lambda_i} (x_n \log \lambda_i - \lambda_i - \log x_n! )\\
&= \sum_{n=1}^N \gamma(s_{ni}) (\frac{x_n}{\lambda_S} - 1)\\
&= 0
\end{align*}
%$$\frac{\partial}{\partial \lambda_i}\mathrm{E}[\log p(x;s,\theta)]$$
%$$=\frac{\partial}{\partial \lambda_i}\sum_{n=1}^N \sum_{s=1}^S
%\mathrm{E}[s_{ni}]\{\log \pi_i + \log \mathcal{P}(x_n|\theta)\}$$
%$$= \sum_{n=1}^N \mathrm{E}[s_{ni}] \frac{\partial}{\partial \lambda_i} \log %\mathcal{P}(x_n|\theta)$$
%$$= \sum_{n=1}^N \gamma(s_{nk}) \frac{\partial}{\partial \lambda_i} \log %{\frac{\lambda^{x_n} e^{-\lambda}}{x_n!}}$$
%$$= \sum_{n=1}^N \gamma(s_{nk}) \frac{\partial}{\partial \lambda_i} (\log %{\lambda_i}^{x_n} + \log e^{-\lambda_k} - \log x_n! )$$
%$$= \sum_{n=1}^N \gamma(s_{ni}) \frac{\partial}{\partial \lambda_i} (x_n \log %\lambda_i - \lambda_i - \log x_n! )$$
%$$= \sum_{n=1}^N \gamma(s_{ni}) (\frac{x_n}{\lambda_S} - 1)= 0$$
\\\\
получаем
\begin{equation}
\lambda_i^* = \frac{1}{N_i}\sum_{n=1}^N\gamma(s_{ni})x_n
\end{equation}
\\\\
где
\begin{equation}
N_i = \sum_{n=1}^N\gamma(s_ni)
\end{equation}
Найдем новые значения априорных вероятностей.
Воспользуемся методом множителей Лагранжа для учета ограничений на вектор априорных вероятностей.

\begin{align*}
\frac{\partial}{\partial \pi_i}( \mathrm{E}[\log p(x;s,\theta)] &+ \lambda(\sum_{j=1}^S \pi_j - 1))
\\&= \frac{\partial}{\partial \pi_i}\sum_{n=1}^N \sum_{i=1}^S
E[s_{ni}]\{\log \pi_i + \log \mathcal{P}(x_n;\theta)\} + \frac{\partial}{\partial \pi_i}\lambda(\sum_{j=1}^S \pi_j - 1)
\\& = \sum_{n=1}^N
\gamma(s_{ni})\frac{\partial}{\partial \pi_i}\{\log \pi_i + \log \mathcal{P}(x_n|\theta)\} + \frac{\partial}{\partial \pi_i} \lambda(\sum_{j=1}^S \pi_j - 1)\\
&=\sum_{n=1}^N
\gamma(s_{ni})\frac{\partial}{\partial \pi_i}\{\log \pi_i\} + \frac{\partial}{\partial \pi_i} \lambda(\sum_{j=1}^S \pi_j - 1) \\
&=\sum_{n=1}^N
\gamma(s_{ni})\frac{1}{\pi_i} + \lambda \\&= 0
\end{align*}
%$$\frac{\partial}{\partial \pi_i}( \mathrm{E}[\log p(x;s,\theta)] + \lambd
%(\sum_{j=1}^S \pi_j - 1))$$
%$$= \frac{\partial}{\partial \pi_i}\sum_{n=1}^N \sum_{i=1}^S
%E[s_{ni}]\{\log \pi_i + \log \mathcal{P}(x_n;\theta)\} + \frac{\partial}{\parti%al \pi_i}\lambda(\sum_{j=1}^S \pi_j - 1)$$
%$$ = \sum_{n=1}^N
%\gamma(s_{ni})\frac{\partial}{\partial \pi_i}\{\log \pi_i + \log \mathcal{P}%(x_n|\theta)\} + \frac{\partial}{\partial \pi_i} \lambda(\sum_{j=1}^S \pi_j - 1%)$$
%$$=\sum_{n=1}^N
%\gamma(s_{ni})\frac{\partial}{\partial \pi_i}\{\log \pi_i\} + \frac{\partial}{\%partial \pi_i} \lambda(\sum_{j=1}^S \pi_j - 1) $$
%$$=\sum_{n=1}^N
%\gamma(s_{ni})\frac{1}{\pi_i} + \lambda = 0$$
\\
Домножив на $\pi_k$, получаем
\begin{equation}
\sum_{n=1}^N \gamma(s_{ni}) + \pi_i \lambda = 0
\end{equation}
Просуммируем вдоль $i$
\begin{equation}
\sum_{n=1}^N \sum_{i=1}^S \gamma(s_{ni}) + \sum_{i=1}^S \pi_i \lambda = 0
\end{equation}
Применяя
$$\sum_{i=1}^S \gamma(s_{ni}) = 1$$
и
$$\sum_{i=1}^S \pi_i = 1$$
Из выражения (7) получаем
$$N + \lambda = 0$$
$$\lambda = -N$$
Далее, подставляя найденное $\lambda$ в выражение (6), получаем
$$\pi_i = - \frac{\sum_{n=1}^N\gamma(s_{ni})}{\lambda} = \frac{N_i}{N}$$
Наконец,
\begin{align*}
\pi_{i}^* = \frac{N_i}{N} 
\end{align*}

Для нахождения наиболее правдоподобной последовательности скрытых состояний достаточно выбрать состояния с наибольшей апостериорной вероятностью для каждого наблюдения:

$$s_n = \argmax_{i \in {1,..,S}^N} {\gamma (s_{ni})} $$

\textbf{Примечание.} Чтобы успешно обучить наши данные, следует правильно проинициализировать начальные значения входных параметров модели. Для этой задачи достаточно использовать алгоритм кластеризации KMeans++\cite{book:murphy}, который "разбивает" наши наблюдения на $S$ кластеров. Работа алгоритма основана на Методе Максимального Правдоподобия. На шаге Е мы определяем для каждого наблюдения ближайший кластер. На шаге М Вычисляем новое значение кластера, которое принимаем за среднее выборочное наблюдений, относящихся к данному кластеру. Алгоритм итерируется до тех пор, пока изменения логарифма правдоподобия не станет меньше $10^{-3}$. В результате, начальные значений параметров Пуасоновского распределения можно принять за значения кластеров.

\subsubsection{Предсказание модели}

В результате работы алгоритма, необходимо предскзать наиболее вероятную послеовательность состояний, породивших наши наблюдения.
Для каждого наблюдения выбирается состояние, соответствующее наибольшей апостериорной вероятности.

\subsection{Скрытая Марковская Модель}

На практике условие о независимости  состояний между соседними наблюдениями в предыдущей модели не выполняется.

Поэтому, перейдем к Скрытой Марковской Модели второго порядка, чтобы учесть зависимость между состояниями соседних наблюдений.

\[
z_{(n+1),i} \mbox{ зависит от } z_{n,i}
\]

Введем понятие базовых состояний: $ S \in \{+, -, \operatorname{null}\}$.
Каждое из базовых состояний описывает ситуацию в одном образце. 

Семантика обозначений следующая:
\\
$(+)$ - сигнал есть,  $(-)$ - шумовый сигнал, $\operatorname{(null)}$ - сигнала нет.
\\\\
\textbf{Замечание.} Отличие $\operatorname{(null)}$ от $(-)$ заключаются в полном отсутсвии сигнала.
\\\\
Для задачи сравнения нам нужно множество состояний, описывающее, что происходит в каждом из образцов, то есть
\\\\
$$S^2 \in {(+,+),(-,-),(\operatorname{null},\operatorname{null}),
(+,-),(-,+),(\operatorname{null},-),(\operatorname{null},+),(-,\operatorname{null}),(+,\operatorname{null})}$$
\\\\
Состояние $S_n$ с одинаковым базовыми состояниями $(+,+),(-,-),(\operatorname{null},\operatorname{null})$ означает, что данные сравниваемых образцов наблюдения $x_n$ похожи между собой.
\\\\

Чтобы задать модель, нужно определить распределения испусканий, то есть $p(x_n|s_{ni} = 1)$, где i — индекс состояния из $S^2$, а n — индекс наблюдения. Будем считать, что $x_n$ — это наблюдение из многомерного распределения Пуассона с независимыми компонентами, то есть:

$$p(x_n|s_{ni}) = \prod\limits_{d = 1}^2 p(x_{nd}|\lambda_{id})$$

На данном этапе для каждого состояния и каждого образца есть свой параметр распределения Пуассона, что является неверной параметизацией. Рассмотрим два состояния $i = (+, +)$ и $j = (+, -)$ и выпишем для них функцию вероятности распределения Пуассона:

\begin{align*}
p(x_n|s_{ni}) &= \prod\limits_{d = 1}^2 p(x_{nd}|\lambda_{id}) = p(x_{n1}|\lambda_{i1}) p(x_{n2}|\lambda_{i2})
\end{align*}
\begin{align*}
p(x_n|s_{nj}) = \prod\limits_{d = 1}^2 p(x_{nd}|\lambda_{jd}) = p(x_{n1}|\lambda_{j1}) p(x_{n2}|\lambda_{j2})
\end{align*}

Первый множитель в обоих выражениях соответствует наблюдению, порождённому базовым состоянием $(+)$ в первом образце. Логично положить, что $\lambda_{i1} = \lambda_{j1}$, потому что в обратном случае испусканиям для одного и того же базового состояния будут соответствовать разные параметры распределения Пуассона. Таким образом, различных лямбд у нас не $2 * |S^2| = 8$, а $2 * |S| = 4$.

Для реализации удобно представлять $\vec{\lambda}$ в виде вектора размерности $2 * |S|$, а для состояния i из $S^2$ использовать матрицу трансляции D.

Матрица трансляции - это двухмерная матрица размерности $2 \times |S^2|$, где $D_{di}$ - индекс лямбды из вектора:

$$\vec{\lambda}=\{\lambda_1, \lambda_2, \lambda_3, \lambda_4\, \lambda_5, \lambda_6\}$$

% Рисунок, размещенный с предпочтением "вверху страницы"
\begin{figure}[h]
\label{разрыв_функции}
\centering
\includegraphics[scale=0.8]{transmat.png}
\caption{Матрица трансляции}
\end{figure}

Перепишем функцию вероятности распределения Пуассона в терминах D.

\begin{equation}p(x_n;s_{ni}) = \prod\limits_{d = 1}^2 \prod\limits_{s = 1}^{|S|} p(x_{nd};\lambda_s)^{I[D_{di} = s]}\end{equation}

Функция правдоподобия определяется как

\begin{equation}p(x,s;\theta) = p(s_1;\pi)\bigg[\prod_{n=2}^N p(s_n;s_{n-1},A)\bigg]\prod_{m=1}^N p(x_n;s_m,\theta)\end{equation}

Подставив определение функции вероятности распределения (7) в функцию правдоподобия для СММ (8) можно убедиться, что М-шаг для вектора лямбд:

\begin{equation}\lambda_s = \frac{\sum\limits_{d = 1}^2 \sum\limits_{i = 1}^{|S|} I[D_{di} = s] \sum\limits_{n = 1}^N \gamma_{ni} x_{nd}}
                 {\sum\limits_{d = 1}^2 \sum\limits_{i = 1}^{|S|} I[D_{di} = s] \sum\limits_{n = 1}^N \gamma_{ni}}\end{equation}

Пусть
\\\\
$\pi = (\pi_1,..,\pi_S^2)$ - априорные вероятности состояний.
\\
$\mathcal{A}$ - матрица вероятностей перехода между состояниями.
\\
$\lambda = (\lambda_1,..,\lambda_S)$ - параметры многомерного распределения Пуасона.
\\\\
% Рисунок, размещенный с предпочтением "вверху страницы"
\begin{figure}[h]
\label{разрыв_функции}
\centering
\includegraphics[scale=0.9]{plate_states3.png}
\caption{решетка состояний}
\end{figure}
\\\\
Решетка представляет собой диаграмму переходов между скрытыми состояниями модели. Красным светом выделены состояния, в котором базовые состояния похожи, то есть $(+,+),(-,-),(\operatorname{null},\operatorname{null})$

\subsection*{Шаг E}
\begin{equation}\gamma_{ni}=\frac{\alpha_{ni}\beta_{ni}}{\sum_{j=1}^S\alpha_{nj}\beta_{nj}}\end{equation}

\begin{equation}\xi_{nij}=\frac{\alpha_{(n-1),i}\mathcal{A}_{ij}\mathcal{P}(x_n;\lambda_j)\beta_{nj}}{\sum_{i'=1}^S\sum_{j'=1}^S\alpha_{(n-1),i'}A_{i'j'}\mathcal{P}(x_n;\lambda_{j'})\beta_{nj'}}\end{equation}
\\\\
Где, 
\\\\
$\alpha_{ni}=p(s_{ni}=1,x_1,x_2,..x_n;\theta)$
\\\\
$\beta_{ni}=p(x_{n+1}, ..,x_N; s_{ni}=1,\theta)$
\\\\
Вычисления $\alpha$ и $\beta$ производится с помощью алгоритма прямого-обратного хода:
\\
$$\alpha_{1i}=\pi_i\mathcal{P}$$
\\
$$\beta_{Ni}=1$$
\\
$$\alpha_{ni}=\mathcal{P}(x_n;\lambda_i)\sum_{j=1}^S\alpha_{(n-1),j}A_{ji}$$
\\
$$\beta_{ni}=\sum_{j=1}^S A_{ij}\mathcal{P}(x_{n+1};\lambda_j)\beta_{(n+1),j}$$
\\\\
\subsection*{Шаг М}
Новые значения параметров модели вычисляются так:
\begin{equation}\pi_i^*=\gamma(s_{1i})\end{equation}

\begin{equation}A_{ij}^*=\frac{\sum_{n=2}^N\xi_{nij}}{\sum_{j'=1}^S\sum_{n=2}^N\xi_{nij}\xi_{nij'}}\end{equation}

\begin{equation}\lambda_i^* = \frac{\sum_{n=1}^N\gamma(s_{ni})x_n}{\sum_{n=1}^N\gamma(s_{ni})}\end{equation}

\textbf{Замечание.} В алгоритме прямого-обратного хода может быть underflow - это значит, что не хватает точности чисел с плавающей точкой.
Поэтому удобно проводить вычисления в логарифмах.

\subsection{Выбор модели}
Для задачи сравнения двух образцов была выбрана Скрытая Марковская Модель.
В частном случае, для анализа одного биологического образца СММ более правдоподобнее, чем смесь Пуасоновских испусканий.
%% запихнуть табличку двухстолбчатую со значениями правдоподобий некоторых хромосом: первый ст. - Смесь, второй - СММ.
Это значит, что модель, в которой предполагается зависимость между состояниями соседних наблюдений, более правдоподобная.

\subsection{Актуальности разработки}

Существуют инструменты для анализа одного эксперимента с использованием FAIRE-seq:

\begin{itemize}
  \item ChromHMM
  \item ZINBA
  \item Fseq
  \item ChIPOTle Peak Finder 
  \item и другие \ldots
\end{itemize}

\textbf{Примечание.} Косвенный аналог ChromHMM моделирует многомерную последовательность из $\{0, 1\}$. Данный инструмент, как и другие аналоги, используется для анализа одного FAIRE-seq образца. Стало быть, для задачи сравнения она не подходит. 
\section{Оценка модели}

\subsection{Оценка качества работы модели}

При оценке качетсва работы модели мы хотели бы получать число неверных предсказаний FDR\cite{wiki:fdr} среди всех предсказаний модели.

Для того чтобы ввести FDR, сформулируем гипотезы, которые будем проверять с помощью модели. 

Для данных FAIRE-seq возможны две гипотезы:
\begin{itemize}
  \item $H_0$ - разницы между состояниями эксперементальных данных в одном наблюдении нет
  \item $H_1$ - разница есть
\end{itemize}

Рассмотрим некоторое наблюдение с индексом i в нашей выборке. Как понять отвергаем ли мы или принимаем для него нулевую гипотезу? 

Воспользуемся для этого апостериорными вероятностями:

\begin{itemize}
  \item $P(\textrm{отличий в i нет}; x, \theta) := p_0$
  \item $P(\textrm{отличия в i есть};x, \theta) := p_1$
\end{itemize}

Нулевая гипотеза отвергается если $p_0 < p_1$ и не отвергается в обратном случае. 

\textbf{Примечание.} Вспомним, что $p_0 + p_1 = 1$, поэтому наш критерий можно записать как:
$$p_0 <= 0.5$$

Таким образом, применив сформулированный выше критерий ко всем бинам $i = 1,..., N$ мы получим N результатов. FDR = $a \in [0, 1]$ означает, что среди N результатов $a * N$ — не верны.

В общем виде FDR записывается так:

$\operatorname{FDR} = \operatorname{E}[\operatorname{FP} / (\operatorname{TP} + \operatorname{FP})]$,

где FP — количество неверно отвергнутых нулевых гипотез, TP — количество верно отвергнутых нулевых гипотез.

Для удобства будем использовать следующую разновидность FDR:

\begin{equation}\operatorname{mFDR} = \operatorname{E}[\operatorname{FP}] / \operatorname{E}[\operatorname{TP} + \operatorname{FP}]\end{equation}

%$$E[FP] = \sum\limits_{i = 1}^N E[FP_i]$$

%Что из себя представляет $FP_i$? Если i-й бин не содержит сигнала, то $FP_i = 0$. Если i-й бин по мнению модели содержит сигнал, то $FP_i = 1$ в случае, когда сигнал там и правда есть и 0 в обратном случае.

%Разумеется, правильного ответа для каждого i мы не знаем, но у нас есть вероятность ошибки, а именно $P(\textrm{отличий в i нет}; x, \theta)$, которую мы обозначили за $p_2$. Посчитаем матожидание:

%$$E[FP] = \sum\limits_{i = 1}^N I[p_2 <= 0.5] * p_2$$

%* тут мы воспользовались тем, что матожидание индикаторной случайной величины с параметром p равняется p.

%Перейдём к знаменателю. Выражение для E[FP] у нас уже есть, построим аналогичное выражение для E[TP].  $TP_i = 1$ если сигнал в i-м бине есть, и модель с этим солидарна, и 0 — во всех остальных случаях. Заметим, что $TP_i$ независимы потому же почему и $FP_i$.

%$$E[TP] = \sum\limits_{i = 1}^N I[p_2 <= 0.5] * p_1$$

%Выпишем то, что получилось:

%$$E[TP + FP] = \sum\limits_{i = 1}^N (I[p_2 <= 0.5] * p_2 + I[p_2 <= 0.5] * p_1) = \sum\limits_{i = 1}^N I[p_2 <= 0.5]$$

%* тут мы воспользовались тем, что $p_1 + p_2 = 1$

%И всё вместе:

%$$mFDR = \frac{ \sum\limits_{i = 1}^N I[p_2 <= 0.5] * p_2}{\sum\limits_{i = 1}^N I[p_2 <= 0.5]}$$

%Таким образом, оценив апостериорные вероятности мы можем также оценить и mFDR. Важно понимать, что по выборке (точнее по апостериорным вероятностям для выборки) мы получили оценку mFDR, а не его истинное значение. 

\subsection{Контроль FDR}

Также мы хотим выдавать вектор предсказаний, в котором гарантированно не более чем фиксированное число ошибок.

% У заключения нет номера главы
\section*{Заключение}

\bibliographystyle{ugost2008ls}
\bibliography{diploma.bib}
\end{document}
